{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c5e8fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/miniconda3/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from modeling.modeling_llada import LLaDAModelLM\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset, load_dataset\n",
    "\n",
    "from llada_get_loglikelihood import forward_process, get_log_likelihood\n",
    "from llada_generate import generate\n",
    "\n",
    "from jinyu_utils.jinyu_tokenizer import Tokenizer_\n",
    "from jinyu_utils.jinyu_preprocess_wiki import parse_lines_with_index, merge_subdocs, PATTEN_REG_WIKI, simple_calculate_sim\n",
    "from jinyu_utils.jinyu_dataset import jinyu_load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6906ae9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_model = 'GSAI-ML/LLaDA-8B-Base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "867f2337",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''load tokenizer'''\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    id_model,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "if tokenizer.padding_side != 'left':\n",
    "    tokenizer.padding_side = 'left'\n",
    "# end\n",
    "\n",
    "assert tokenizer.pad_token_id != 126336"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8633db23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "Loading checkpoint shards: 100%|██████████| 6/6 [00:02<00:00,  2.96it/s]\n"
     ]
    }
   ],
   "source": [
    "'''load model'''\n",
    "model_kwargs = {}\n",
    "model = LLaDAModelLM.from_pretrained(\n",
    "    id_model,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    **model_kwargs\n",
    ")\n",
    "\n",
    "model = model.eval()\n",
    "device_for_input = model.get_input_embeddings().weight.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ea035bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''load dataset'''\n",
    "ds = jinyu_load_dataset(2)['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e25faf3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0]['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "217ba359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prefix': \"Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\", 'target': 'Janet sells 16 - 3 - 4 = <<16-3-4=9>>9 duck eggs a day.\\nShe makes 9 * 2 = $<<9*2=18>>18 every day at the farmer’s market.\\n#### 18'}\n"
     ]
    }
   ],
   "source": [
    "samples = []\n",
    "for data in ds:\n",
    "    samples.append({'prefix': data['question'], 'target': data['answer']})\n",
    "# end\n",
    "samples = samples[:50]\n",
    "print(samples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2963bfa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "starting to get outputs...: 100%|██████████| 50/50 [00:43<00:00,  1.16it/s]\n"
     ]
    }
   ],
   "source": [
    "prompts = [sample['prefix'] for sample in samples]\n",
    "outputs = []\n",
    "with torch.no_grad():\n",
    "    for prompt in tqdm(prompts, desc='starting to get outputs...'):\n",
    "        encoded_inputs = tokenizer(\n",
    "                prompt,\n",
    "                add_special_tokens=False,\n",
    "                padding=True,\n",
    "                return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        input_ids = encoded_inputs['input_ids'].to(device_for_input)\n",
    "        attention_mask = encoded_inputs['attention_mask'].to(device_for_input)\n",
    "\n",
    "        out = generate(model, input_ids, attention_mask, steps=32, gen_length=128, block_length=128, temperature=0., cfg_scale=0., remasking='low_confidence')\n",
    "        output = tokenizer.batch_decode(out[:, input_ids.shape[1]:], skip_special_tokens=True)[0]\n",
    "        outputs.append(output)\n",
    "    # end for\n",
    "# end with\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8558c664",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('gsm8k_base_50_32', 'w+') as file:\n",
    "    idx = 0\n",
    "    contents = []\n",
    "\n",
    "    for sample, predict in zip(samples, outputs):\n",
    "        content = {\n",
    "            'prefix': sample['prefix'],\n",
    "            'target': sample['target'],\n",
    "            'predict': predict\n",
    "        }\n",
    "\n",
    "        contents.append(content)\n",
    "\n",
    "        idx += 1\n",
    "    # end\n",
    "\n",
    "    file.write(json.dumps(contents, indent=4))\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1473e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''get log likelihood parts'''\n",
    "\n",
    "\n",
    "# class Tokenizer_test(Tokenizer_):\n",
    "#     def _tokenize(self, e):\n",
    "#         prefix, target = self._encode_pair(e['prefix'], e['target'])\n",
    "#         return {\n",
    "#             'prefix_text': e['prefix'],\n",
    "#             'target_text': e['target'],\n",
    "#             'prefix': prefix,\n",
    "#             'target': target\n",
    "#         }\n",
    "#     # end\n",
    "# # end\n",
    "\n",
    "\n",
    "# ds =  Dataset.from_list(samples)\n",
    "# ds = ds.map(Tokenizer_test(tokenizer))\n",
    "# ds = ds.with_format(\"torch\")\n",
    "# ds = ds.filter(lambda x: len(x[\"prefix\"]) + len(x['target']) <= 4096)\n",
    "\n",
    "# out = []\n",
    "# with torch.no_grad():\n",
    "#     for elem in tqdm(ds, desc=\"Computing likelihood...\"):\n",
    "#         prefix = elem[\"prefix\"]\n",
    "#         target = elem[\"target\"]\n",
    "\n",
    "#         ll = get_log_likelihood(model, prefix, target)\n",
    "#         out.append(ll)\n",
    "#     # end\n",
    "# # end\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
