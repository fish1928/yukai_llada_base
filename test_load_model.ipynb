{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67c5e8fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/miniconda3/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from modeling.modeling_llada import LLaDAModelLM\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset, load_dataset\n",
    "\n",
    "from get_log_likelihood import forward_process, get_log_likelihood\n",
    "from generate import generate\n",
    "\n",
    "from jinyu_utils.jinyu_tokenizer import Tokenizer_\n",
    "from jinyu_utils.jinyu_preprocess_wiki import parse_lines_with_index, merge_subdocs, PATTEN_REG_WIKI, simple_calculate_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6906ae9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/exx/hf_hub_cache/models--GSAI-ML--LLaDA-8B-Base/snapshots/0f2787f2d87eac5eed8a087d5ecd24277e6255b2\n"
     ]
    }
   ],
   "source": [
    "id_model = 'GSAI-ML/LLaDA-8B-Base'\n",
    "path_cache_base = os.environ['HF_HUB_CACHE']\n",
    "folder_model = '--'.join(['models'] + id_model.split('/'))\n",
    "path_cache_model = os.path.join(path_cache_base, folder_model)\n",
    "path_snapshot_model = os.path.join(path_cache_model, 'snapshots')\n",
    "folder_snapshot_model_1 = [entity for entity in os.listdir(path_snapshot_model) if entity[0] != '.'][0]\n",
    "path_snapshot_model_1 = os.path.join(path_snapshot_model, folder_snapshot_model_1)\n",
    "print(path_snapshot_model_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "867f2337",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''load tokenizer'''\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    path_snapshot_model_1,\n",
    "    local_files_only=True,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "if tokenizer.padding_side != 'left':\n",
    "    tokenizer.padding_side = 'left'\n",
    "# end\n",
    "\n",
    "assert tokenizer.pad_token_id != 126336"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8633db23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "Loading checkpoint shards: 100%|██████████| 6/6 [00:03<00:00,  1.59it/s]\n"
     ]
    }
   ],
   "source": [
    "'''load model'''\n",
    "model_kwargs = {}\n",
    "model = LLaDAModelLM.from_pretrained(\n",
    "    path_snapshot_model_1,\n",
    "    local_files_only=True,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    **model_kwargs\n",
    ")\n",
    "\n",
    "model = model.eval()\n",
    "device_for_input = model.get_input_embeddings().weight.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ea035bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''load dataset'''\n",
    "names_dataset = [('Idavidrein/gpqa', 'gpqa_main'), ('Salesforce/wikitext', 'wikitext-2-raw-v1')]\n",
    "ds = load_dataset(*names_dataset[1], split='train')['text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9b96a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''preprocess dataset'''\n",
    "docs, _ = parse_lines_with_index(PATTEN_REG_WIKI, ds)\n",
    "docs = docs['subdocs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "217ba359",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = []\n",
    "for doc in docs:\n",
    "    lines_1 = doc['texts']\n",
    "    paragraph_1 = ' '.join(lines_1)\n",
    "    lines_remain, titles = merge_subdocs(doc['subdocs'])\n",
    "    paragraph_remain = ' '.join(lines_remain)\n",
    "    prefix = paragraph_1\n",
    "    target = paragraph_remain\n",
    "    samples.append({'prefix': prefix, 'target': target})\n",
    "# end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc4d33de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = samples[:100]\n",
    "len(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2963bfa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "starting to get outputs...:  50%|█████     | 50/100 [05:17<05:45,  6.91s/it]"
     ]
    }
   ],
   "source": [
    "prompts = [sample['prefix'] for sample in samples]\n",
    "outputs = []\n",
    "with torch.no_grad():\n",
    "    for prompt in tqdm(prompts, desc='starting to get outputs...'):\n",
    "        encoded_inputs = tokenizer(\n",
    "                prompt,\n",
    "                add_special_tokens=False,\n",
    "                padding=True,\n",
    "                return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        input_ids = encoded_inputs['input_ids'].to(device_for_input)\n",
    "        attention_mask = encoded_inputs['attention_mask'].to(device_for_input)\n",
    "\n",
    "        out = generate(model, input_ids, attention_mask, steps=128, gen_length=128, block_length=32, temperature=0., cfg_scale=0., remasking='low_confidence')\n",
    "        output = tokenizer.batch_decode(out[:, input_ids.shape[1]:], skip_special_tokens=True)[0]\n",
    "        outputs.append(output)\n",
    "    # end for\n",
    "# end with\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ffa399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "= = Background and development = = The Dangerously In Love Tour was the debut solo concert tour by American recording artist Beyoncé . The tour was intended to showcase songs from Beyoncé ' debut solo album , Dangerously in Love released in 2003 . However , the set list also contained a special segment of her show dedicated to her girl group Destiny 's Child and songs from Beyoncé ' 2003 film The Fighting Temptations ( \" Fever \" and \" Summertime \" ) . The stage was simple and featured a large LED screen in the back that moved up and down throughout the entire show and displayed video images of Beyoncé and her dancers , as well as some images from her music videos and some prerecorded images with special effects . The show also featured a small staircase and platforms on both side of the stairs for her band . Beyoncé later toured alongside Missy Elliott and Alicia Keys as ensemble for the Verizon Ladies First Tour ( 2004 ) in North America . = = Synopsis and reception = = Dave Simpson of The Guardian described the opening of the show during his review : \" Some while after Beyoncé is due on stage , a voice announces that the support act won 't be appearing and that Beyoncé will be with us ' in a moment ' . Like everything else – hits , boots , hair and sponsorship deals – moments are very big in Beyoncé world . An age later , cheers erupt for the raising of a curtain which revealed , er , a roadie fiddling with a drum kit . An hour later , the piped music is getting gradually louder to drown boos and the cries of small children whose parents are moaning it 's getting past their bedtime . \" The show opens with \" Baby Boy \" which Beyoncé sang while being lowered onto the stage upside down . A highlight for many fans was her performance of \" Dangerously in Love 2 \" . During the tour , a special 8 @-@ minutes rendition of the song was performed . Simpson of The Guardian reviewed the opening show of the tour negatively , grading it with two out of five stars . He was negative about Beyoncé ' clothing during the show , saying : \" The delays may well be down to Beyoncé 's wardrobe , which could trouble Imelda Marcos . There are skimpy skirts , tails ( for a note perfect if pointless version of Peggy Lee 's Fever ) and a general theme of low material , high glitz . But often , the main sparkle is on Beyoncé 's outfit . \" He also added that \" The dancers ' ' naked suits ' make the former church girl a raunchy rival to Kylie [ Minogue ] . But there 's an interminable section where they pretend to be homies , and when Beyoncé disappears for long periods it feels like an expensive night with Legs and Co . \" He concluded his review by saying , \" Clearly , the armies of industry professionals that put Beyoncé together aren 't sure of her core audience . A vague Saturday night TV , family entertainment feel gradually gives way to a more intriguing cross between Liza Minelli showbiz and thumping R & B. However , a ticker tape festooned Crazy In Love and a belting Work It Out suggest Beyoncé is best sticking to her roots . Bizarrely , if implausibly , she puts the carnage down to her tour manager falling off stage , but at least she 's grasped one showbiz adage : the show must go on . \" = = Broadcasts and recordings = = On November 10 , 2003 , Beyoncé performed at the Wembley Arena in London ; this was later put on a DVD , titled Live at Wembley , which was released in April 2004 . It was accompanied by a CD comprising three previously @-@ unreleased studio recorded songs and one remix each of \" Crazy in Love \" , \" Baby Boy \" and \" Naughty Girl \" . Behind @-@ the @-@ scenes footage can be also seen on the DVD . The album debuted at number seventeen on the Billboard 200 , selling 45 @,@ 000 copies in its first week . The DVD has been certified double platinum by the Recording Industry Association of America for shipping 200 @,@ 000 copies . According to Nielsen SoundScan , it had sold 264 @,@ 000 copies in the US by October 2007 , while as at October 6 , 2010 , it had sold 197 @,@ 000 digital downloads . In an interview with The New York Times in 2007 , American singer Miranda Lambert revealed that Live at Wembley inspired her to \" take little bits from that [ Beyoncé ' performance ] \" for her live shows . = = Set list = = \" Baby Boy \" \" Naughty Girl \" \" Fever \" \" Hip Hop Star \" \" Yes \" \" Work It Out \" \" Gift from Virgo \" \" Be with You \" \" Speechless \" Destiny 's Child Medley : \" Bug a Boo \" \" No , No , No Part 2 \" \" Bootylicious \" \" Jumpin ' , Jumpin ' \" \" Say My Name \" \" Independent Women Part I \" \" ' 03 Bonnie & Clyde \" \" Survivor \" \" Me , Myself and I \" \" Summertime \" \" Dangerously in Love 2 \" \" Crazy in Love \" = = Tour dates = =\n",
      " The tour was followed by Beyoncé 's second concert tour , the Beyoncé Experience Tour ( 2004 -- 2005 ) .\n",
      "Output:\n",
      "1: The Dangerously in Love Tour was the debut concert tour by American recording artist Beyoncé. Although the tour was intended to showcase songs from her debut solo album, Dangerously in Love, (2003) the set list also contained a special segment dedicated to Beyoncé's girl group Destiny's Child and featured songs from her 2003 film The Fighting Temptations. The stage was simple and featured a large\n"
     ]
    }
   ],
   "source": [
    "sims_target = []\n",
    "for sample, predict in zip(samples, outputs):\n",
    "    sims_target.append(simple_calculate_sim(sample['target'], predict[0]))\n",
    "# end\n",
    "sims_target = [(idx, sim) for idx, sim in enumerate(sims_target)]\n",
    "sims_target_sorted = sorted(sims_target, key=lambda copus: -copus[1])\n",
    "sims_target_sorted[:10]\n",
    "\n",
    "print(samples[91]['target'])\n",
    "print(outputs[91][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cf79da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "= Soviet cruiser Krasnyi Kavkaz = Krasnyi Kavkaz ( from Russian : \" Красный Кавказ \" - \" Red Caucasus \" ) was a cruiser of the Soviet Navy that began construction during World War I , but was still incomplete during the Russian Revolution . Her design was heavily modified by the Soviets and she was completed in 1932 . During World War II she supported Soviet troops during the Siege of Odessa , Siege of Sevastopol , and the Kerch @-@ Feodosiya Operation in the winter of 1941 — 42 . She was awarded the Guards title on 3 April 1942 . She was reclassified as a training ship in May 1947 before being used as a target in 1952 .\n",
      " = Soviet cruiser Krasnyi Kavkaz = Krasnyi Kavkaz ( from Russian : \" Красный Кавказ \" - \" Red Caucasus \" ) was a cruiser of the Soviet Navy that began construction during World War I , but was still incomplete during the Russian Revolution . Her design was heavily modified by the Soviets and she was completed in 1932 . During World War II she supported Soviet troops during the Siege of Odessa , Siege of Sevastopol , and the Kerch @-@ Feodosiya Operation in the winter of 1941 — 4\n"
     ]
    }
   ],
   "source": [
    "sims_prefix = []\n",
    "for sample, predict in zip(samples, outputs):\n",
    "    sims_prefix.append(simple_calculate_sim(sample['prefix'], predict[0]))\n",
    "# end\n",
    "sims_prefix = [(idx, sim) for idx, sim in enumerate(sims_prefix)]\n",
    "sims_prefix_sorted = sorted(sims_prefix, key=lambda copus: -copus[1])\n",
    "sims_prefix_sorted[:10]\n",
    "print(samples[43]['prefix'])\n",
    "print(outputs[43][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9e3b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "starting to get outputs...: 100%|██████████| 1/1 [00:01<00:00,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "= Soviet cruiser Krasnyi Kavkaz = Krasnyi Kavkaz ( from Russian : \" Красный Кавказ \" - \" Red Caucasus \" ) was a cruiser of the Soviet Navy that began construction during World War I , but was still incomplete during the Russian Revolution . Her design was heavily modified by the Soviets and she was completed in 1932 . During World War II she supported Soviet troops during the Siege of Odessa , Siege of Sevastopol , and the Kerch @-@ Feodosiya Operation in the winter of 1941 — 42 . She was awarded the Guards title on 3 April 1942 . She was reclassified as a training ship in May 1947 before being used as a target in 1952 .\n",
      " ==i=============================================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''one-by-one testing'''\n",
    "idx = 43\n",
    "prompts = [samples[idx]['prefix']]\n",
    "with torch.no_grad():\n",
    "    for prompt in tqdm(prompts, desc='starting to get outputs...'):\n",
    "        encoded_inputs = tokenizer(\n",
    "                prompt,\n",
    "                add_special_tokens=False,\n",
    "                padding=True,\n",
    "                return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        input_ids = encoded_inputs['input_ids'].to(device_for_input)\n",
    "        attention_mask = encoded_inputs['attention_mask'].to(device_for_input)\n",
    "\n",
    "        out = generate(model, input_ids, attention_mask, steps=32, gen_length=128, block_length=32, temperature=0., cfg_scale=0., remasking='low_confidence')\n",
    "        output = tokenizer.batch_decode(out[:, input_ids.shape[1]:], skip_special_tokens=True)\n",
    "        print(simple_calculate_sim(samples[idx]['prefix'], output[0]))\n",
    "        print(samples[idx]['prefix'])\n",
    "        print(output[0])\n",
    "    # end for\n",
    "# end with\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1473e0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'get log likelihood parts'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''get log likelihood parts'''\n",
    "\n",
    "# samples = []\n",
    "# for doc in docs:\n",
    "#     lines_1 = doc['texts']\n",
    "#     paragraph_1 = ' '.join(lines_1)\n",
    "#     lines_remain, titles = merge_subdocs(doc)\n",
    "#     paragraph_remain = ' '.join(lines_remain)\n",
    "#     prefix = 'I will give you a general description of a person. I will also give you some subtitles and you need to give me the detail of them respectively . '\n",
    "#     prefix += paragraph_1\n",
    "#     prefix += \" Titles are : \"\n",
    "#     prefix += ' , '.join(titles)\n",
    "\n",
    "#     target = paragraph_remain\n",
    "#     samples.append({'prefix': prefix, 'target': target})\n",
    "# # end\n",
    "\n",
    "# class Tokenizer_test(Tokenizer_):\n",
    "#     def _tokenize(self, e):\n",
    "#         prefix, target = self._encode_pair(e['prefix'], e['target'])\n",
    "#         return {\n",
    "#             'prefix_text': e['prefix'],\n",
    "#             'target_text': e['target'],\n",
    "#             'prefix': prefix,\n",
    "#             'target': target\n",
    "#         }\n",
    "#     # end\n",
    "# # end\n",
    "\n",
    "\n",
    "# ds =  Dataset.from_list(samples)\n",
    "# ds = ds.map(Tokenizer_test(tokenizer))\n",
    "# ds = ds.with_format(\"torch\")\n",
    "# ds = ds.filter(lambda x: len(x[\"prefix\"]) + len(x['target']) <= 4096)\n",
    "\n",
    "# out = []\n",
    "# with torch.no_grad():\n",
    "#     for elem in tqdm(ds, desc=\"Computing likelihood...\"):\n",
    "#         prefix = elem[\"prefix\"]\n",
    "#         target = elem[\"target\"]\n",
    "\n",
    "#         ll = get_log_likelihood(model, prefix, target)\n",
    "#         out.append(ll)\n",
    "#     # end\n",
    "# # end\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
