{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67c5e8fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/miniconda3/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import DatasetBuilder\n",
    "from torch.cuda import nvtx\n",
    "\n",
    "from modeling_fastdllm.modeling_llada import LLaDAModelLM\n",
    "from fastdllm_get_loglikelihood import get_loglikelihood\n",
    "from fastdllm_generate import generate, generate_with_dual_cache\n",
    "\n",
    "from jinyu_utils.jinyu_dataset import jinyu_load_dataset\n",
    "from jinyu_utils.jinyu_tokenizer import Tokenizer_\n",
    "from jinyu_utils.jinyu_preprocess_wiki import parse_lines_with_index, merge_subdocs, PATTEN_REG_WIKI, simple_calculate_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6906ae9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_model = 'GSAI-ML/LLaDA-8B-Instruct'\n",
    "# path_cache_base = os.environ['HF_HUB_CACHE']\n",
    "# folder_model = '--'.join(['models'] + id_model.split('/'))\n",
    "# path_cache_model = os.path.join(path_cache_base, folder_model)\n",
    "# path_snapshot_model = os.path.join(path_cache_model, 'snapshots')\n",
    "# folder_snapshot_model_1 = [entity for entity in os.listdir(path_snapshot_model) if entity[0] != '.'][0]\n",
    "# path_snapshot_model_1 = os.path.join(path_snapshot_model, folder_snapshot_model_1)\n",
    "# print(path_snapshot_model_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "867f2337",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''load tokenizer'''\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    id_model,\n",
    "    # local_files_only=True,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "if tokenizer.padding_side != 'left':\n",
    "    tokenizer.padding_side = 'left'\n",
    "# end\n",
    "\n",
    "assert tokenizer.pad_token_id != 126336"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8633db23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "Loading checkpoint shards: 100%|██████████| 6/6 [00:02<00:00,  2.32it/s]\n"
     ]
    }
   ],
   "source": [
    "'''load model'''\n",
    "model_kwargs = {}\n",
    "model = LLaDAModelLM.from_pretrained(\n",
    "    id_model,\n",
    "    # local_files_only=True,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    **model_kwargs\n",
    ")\n",
    "\n",
    "model = model.eval()\n",
    "device_for_input = model.get_input_embeddings().weight.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ea035bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''load dataset'''\n",
    "ds = jinyu_load_dataset(1, split='test')['text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9b96a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''preprocess dataset'''\n",
    "docs, _ = parse_lines_with_index(PATTEN_REG_WIKI, ds)\n",
    "docs = docs['subdocs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "217ba359",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = []\n",
    "for doc in docs:\n",
    "    lines_1 = doc['texts']\n",
    "    paragraph_1 = ' '.join(lines_1)\n",
    "    lines_remain, titles = merge_subdocs(doc['subdocs'])\n",
    "    paragraph_remain = ' '.join(lines_remain)\n",
    "    prefix = paragraph_1\n",
    "    target = paragraph_remain\n",
    "    samples.append({'prefix': prefix, 'target': target})\n",
    "# end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc4d33de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = samples[:5]\n",
    "len(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2963bfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Lily can run 12 kilometers per hour for 4 hours. After that, she runs 6 kilometers per hour. How many kilometers can she run in 8 hours?\"\n",
    "\n",
    "# Add special tokens for the Instruct model. The Base model does not require the following two lines.\n",
    "m = [{\"role\": \"user\", \"content\": prompt}]\n",
    "prompt = tokenizer.apply_chat_template(m, add_generation_prompt=True, tokenize=False)\n",
    "\n",
    "input_ids = tokenizer(prompt)['input_ids']\n",
    "input_ids = torch.tensor(input_ids).to(device_for_input).unsqueeze(0)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    nvtx.range_push(\"INFER\")\n",
    "    out = generate_with_dual_cache(model, input_ids, steps=64, gen_length=128, block_length=32, temperature=0., remasking='low_confidence')\n",
    "    torch.cuda.synchronize()\n",
    "    nvtx.range_pop()\n",
    "# end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "878c6843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lily can run 12 kilometers per hour for 4 hours, so she runs a total of 12 kilometers per hour x 4 hours = 48 kilometers.\n",
      "After that, she runs 6 kilometers per hour for the remaining 4 hours, so she runs a total of 6 kilometers per hour x 4 hours = 24 kilometers.\n",
      "Therefore, Lily can run a total of 48 kilometers + 24 kilometers = 72 kilometers in 8 hours.\n",
      "Conclusively: 72\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.batch_decode(out[0][:, input_ids.shape[1]:], skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0439f2a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 201])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14ffa399",
   "metadata": {},
   "outputs": [],
   "source": [
    "sims_target = []\n",
    "for sample, predict in zip(samples, outputs):\n",
    "    sims_target.append(simple_calculate_sim(sample['target'], predict))\n",
    "# end\n",
    "sims_target = [(idx, sim) for idx, sim in enumerate(sims_target)]\n",
    "sims_target_sorted = sorted(sims_target, key=lambda copus: -copus[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5cf79da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(16, 0.9876543209876543),\n",
       " (0, 0.9807692307692307),\n",
       " (15, 0.8),\n",
       " (37, 0.6071428571428571),\n",
       " (35, 0.578125),\n",
       " (48, 0.5),\n",
       " (31, 0.4583333333333333),\n",
       " (5, 0.36231884057971014),\n",
       " (23, 0.3333333333333333),\n",
       " (32, 0.288135593220339)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims_prefix = []\n",
    "for sample, predict in zip(samples, outputs):\n",
    "    sims_prefix.append(simple_calculate_sim(sample['prefix'], predict))\n",
    "# end\n",
    "sims_prefix = [(idx, sim) for idx, sim in enumerate(sims_prefix)]\n",
    "sims_prefix_sorted = sorted(sims_prefix, key=lambda copus: -copus[1])\n",
    "sims_prefix_sorted[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "211f8d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('wikiraw_base_50_32', 'w+') as file:\n",
    "    idx = 0\n",
    "    contents = []\n",
    "\n",
    "    for sample, predict in zip(samples, outputs):\n",
    "        content = {\n",
    "            'prefix': sample['prefix'],\n",
    "            'target': sample['target'],\n",
    "            'predict': predict,\n",
    "            'sim_prefix': sims_prefix[idx][1],\n",
    "            'sim_target': sims_target[idx][1]\n",
    "        }\n",
    "\n",
    "        contents.append(content)\n",
    "\n",
    "        idx += 1\n",
    "    # end\n",
    "\n",
    "    file.write(json.dumps(contents, indent=4))\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f9e3b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''one-by-one testing'''\n",
    "# idx = 43\n",
    "# prompts = [samples[idx]['prefix']]\n",
    "# with torch.no_grad():\n",
    "#     for prompt in tqdm(prompts, desc='starting to get outputs...'):\n",
    "#         encoded_inputs = tokenizer(\n",
    "#                 prompt,\n",
    "#                 add_special_tokens=False,\n",
    "#                 padding=True,\n",
    "#                 return_tensors=\"pt\"\n",
    "#         )\n",
    "\n",
    "#         input_ids = encoded_inputs['input_ids'].to(device_for_input)\n",
    "#         attention_mask = encoded_inputs['attention_mask'].to(device_for_input)\n",
    "\n",
    "#         out = generate(model, input_ids, attention_mask, steps=32, gen_length=128, block_length=32, temperature=0., cfg_scale=0., remasking='low_confidence')\n",
    "#         output = tokenizer.batch_decode(out[:, input_ids.shape[1]:], skip_special_tokens=True)\n",
    "#         print(simple_calculate_sim(samples[idx]['prefix'], output[0]))\n",
    "#         print(samples[idx]['prefix'])\n",
    "#         print(output[0])\n",
    "#     # end for\n",
    "# # end with\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1473e0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'get log likelihood parts'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''get log likelihood parts'''\n",
    "\n",
    "# samples = []\n",
    "# for doc in docs:\n",
    "#     lines_1 = doc['texts']\n",
    "#     paragraph_1 = ' '.join(lines_1)\n",
    "#     lines_remain, titles = merge_subdocs(doc)\n",
    "#     paragraph_remain = ' '.join(lines_remain)\n",
    "#     prefix = 'I will give you a general description of a person. I will also give you some subtitles and you need to give me the detail of them respectively . '\n",
    "#     prefix += paragraph_1\n",
    "#     prefix += \" Titles are : \"\n",
    "#     prefix += ' , '.join(titles)\n",
    "\n",
    "#     target = paragraph_remain\n",
    "#     samples.append({'prefix': prefix, 'target': target})\n",
    "# # end\n",
    "\n",
    "# class Tokenizer_test(Tokenizer_):\n",
    "#     def _tokenize(self, e):\n",
    "#         prefix, target = self._encode_pair(e['prefix'], e['target'])\n",
    "#         return {\n",
    "#             'prefix_text': e['prefix'],\n",
    "#             'target_text': e['target'],\n",
    "#             'prefix': prefix,\n",
    "#             'target': target\n",
    "#         }\n",
    "#     # end\n",
    "# # end\n",
    "\n",
    "\n",
    "# ds =  Dataset.from_list(samples)\n",
    "# ds = ds.map(Tokenizer_test(tokenizer))\n",
    "# ds = ds.with_format(\"torch\")\n",
    "# ds = ds.filter(lambda x: len(x[\"prefix\"]) + len(x['target']) <= 4096)\n",
    "\n",
    "# out = []\n",
    "# with torch.no_grad():\n",
    "#     for elem in tqdm(ds, desc=\"Computing likelihood...\"):\n",
    "#         prefix = elem[\"prefix\"]\n",
    "#         target = elem[\"target\"]\n",
    "\n",
    "#         ll = get_log_likelihood(model, prefix, target)\n",
    "#         out.append(ll)\n",
    "#     # end\n",
    "# # end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa1fc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompts = [sample['prefix'] for sample in samples]\n",
    "# outputs = []\n",
    "# with torch.no_grad():\n",
    "#     for prompt in tqdm(prompts, desc='starting to get outputs...'):\n",
    "#         encoded_inputs = tokenizer(\n",
    "#                 prompt,\n",
    "#                 add_special_tokens=False,\n",
    "#                 padding=True,\n",
    "#                 return_tensors=\"pt\"\n",
    "#         )\n",
    "\n",
    "#         input_ids = encoded_inputs['input_ids'].to(device_for_input)\n",
    "#         attention_mask = encoded_inputs['attention_mask'].to(device_for_input)\n",
    "\n",
    "#         out = generate(model, input_ids, attention_mask, steps=32, gen_length=128, block_length=32, temperature=0., cfg_scale=0., remasking='low_confidence')\n",
    "#         output = tokenizer.batch_decode(out[:, input_ids.shape[1]:], skip_special_tokens=True)[0]\n",
    "#         outputs.append(output)\n",
    "#     # end for\n",
    "# # end with\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
